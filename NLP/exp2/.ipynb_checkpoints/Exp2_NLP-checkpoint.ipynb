{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IK9RuIfjExhZ"
   },
   "source": [
    "Aim : To perform the text processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOfkkjSmKCZL"
   },
   "source": [
    "#Installing Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mO3XJZLWKBfX",
    "outputId": "1a0c1d10-0246-4e28-bc3d-a6164a97b9f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# Basic Preprocessing\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmu0EqzlJZtB"
   },
   "source": [
    "#Tokenization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80X7UX4hJcVC"
   },
   "source": [
    "##Tokenization is the process by which a big quantity of text is divided into smaller parts called tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN9jIutgJhSE"
   },
   "source": [
    "## Word Tokenization : This involves splitting a sentence into words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYpBEztUJMdj",
    "outputId": "7f345441-c64b-411b-e597-deec29ce7d6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Sentence Tokenization: ['This', 'is', 'a', 'test', 'statement', 'for', 'tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "#sentence = input('Enter a sentence: ')\n",
    "sentence= \"This is a test statement for tokenization.\"\n",
    "tokenized_sent = nltk.tokenize.word_tokenize(sentence)\n",
    "print(f'After Sentence Tokenization: {tokenized_sent}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDzFMChl7Skh"
   },
   "source": [
    "# Sentence Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLObZxVw7jGu",
    "outputId": "679a4dbb-5a80-49f6-b856-aa97103c89d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This is a test statement for tokenization.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"This is a test statement for tokenization.\"\n",
    "print(sent_tokenize(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B6px1OOO-tyt"
   },
   "source": [
    "#Stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UtuBCx4J-wVQ"
   },
   "source": [
    "##Stemming is a kind of normalization where a set of words in a sentence are converted into a sequence to shorten its lookup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vAR48zV--5qn",
    "outputId": "85f14beb-2f99-4793-dc68-e61e9b5e529c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stemming: ['thi', 'is', 'a', 'test', 'statement', 'for', 'token', '.']\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "print(f'After Stemming: {[stemmer.stem(x) for x in tokenized_sent]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VK6wBMz_unw"
   },
   "source": [
    "Example01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4E586XoMIGWh",
    "outputId": "f207671f-2bee-4db6-b315-19fa21b2f70f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stemming: ['In', 'comput', ',', 'the', 'term', 'text', 'process', 'refer', 'to', 'the', 'theori', 'and', 'practic', 'of', 'autom', 'the', 'creation', 'or', 'manipul', 'of', 'electron', 'text', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sent = nltk.tokenize.word_tokenize(\"In computing, the term text processing refers to the theory and practice of automating the creation or manipulation of electronic text. \")\n",
    "print(f'After Stemming: {[stemmer.stem(x) for x in tokenized_sent]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-f55OlSx_wjT"
   },
   "source": [
    "Example02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7WknKcWIG9N",
    "outputId": "1deb81da-993a-439d-e39d-097020ea7c9f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stemming: ['the', 'text', 'process', 'of', 'a', 'regular', 'express', 'is', 'a', 'virtual', 'edit', 'machin']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sent = nltk.tokenize.word_tokenize(\"The text processing of a regular expression is a virtual editing machine\")\n",
    "print(f'After Stemming: {[stemmer.stem(x) for x in tokenized_sent]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZcMWQiqH_458"
   },
   "source": [
    "Example03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N8A2W15bIHqp",
    "outputId": "10f0fcb4-7256-4981-ebe0-b60d4f613774"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stemming: ['text', 'process', 'involv', 'comput', 'command', 'which', 'invok', 'content', ',', 'content', 'chang', ',', 'and', 'cursor', 'movement']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sent = nltk.tokenize.word_tokenize(\"Text processing involves computer commands which invoke content, content changes, and cursor movement\")\n",
    "print(f'After Stemming: {[stemmer.stem(x) for x in tokenized_sent]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zvw4TFIm_6wr",
    "outputId": "d34243b4-8459-4052-8e34-0b3bf1f6cfd4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recent\n",
      "research\n",
      "report\n",
      "that\n",
      "the\n",
      "viru\n",
      "can\n",
      "remain\n",
      "viabl\n",
      "for\n",
      "up\n",
      "to\n",
      "72\n",
      "hour\n",
      "on\n",
      "plastic\n",
      "and\n",
      "stainless\n",
      "steel\n",
      "and\n",
      "up\n",
      "to\n",
      "24\n",
      "hour\n",
      "on\n",
      "cardboard\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "sentence=\"Recent researchs reported that the virus can remain viable for up to 72 hours on plastic and stainless steel and up to 24 hours on cardboard.\"\n",
    "words = word_tokenize(sentence)\n",
    "ps = PorterStemmer()\n",
    "for w in words:\n",
    "\trootWord=ps.stem(w)\n",
    "\tprint(rootWord)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hw5GMArm_QC6"
   },
   "source": [
    "# Lemmatization\n",
    "\n",
    "## Lemmatization is the algorithmic process of finding the lemma of a word depending on their meaning. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A6ZNCgTqBwIv"
   },
   "source": [
    "Example01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o9CS1HRPBaFF",
    "outputId": "1ae6dd07-d7dc-4cca-cfec-5773adbf0490"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Lemmatization: ['Text', 'processing', 'involves', 'computer', 'command', 'which', 'invoke', 'content', ',', 'content', 'change', ',', 'and', 'cursor', 'movement']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "print(f'After Lemmatization: {[lemmatizer.lemmatize(x) for x in tokenized_sent]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eE7PS7SEBzGK"
   },
   "source": [
    "Example02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j2SYr-48Iusv",
    "outputId": "ac77462f-bb57-4a78-f76c-f0ab48653493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Lemmatization: ['Text', 'processing', 'involves', 'computer', 'command', 'which', 'invoke', 'content', ',', 'content', 'change', ',', 'and', 'cursor', 'movement']\n"
     ]
    }
   ],
   "source": [
    "tokenized_sent = nltk.tokenize.word_tokenize(\"Text processing involves computer commands which invoke content, content changes, and cursor movement\")\n",
    "print(f'After Lemmatization: {[lemmatizer.lemmatize(x) for x in tokenized_sent]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1UiehB8B2E6"
   },
   "source": [
    "#Stop Word Removal\n",
    "\n",
    "##A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPrYHdS5CChF",
    "outputId": "0258ab48-c169-4502-e0c2-ed6d8b5142c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing Stopwords: ['Text', 'processing', 'involves', 'computer', 'commands', 'invoke', 'content', ',', 'content', 'changes', ',', 'cursor', 'movement']\n"
     ]
    }
   ],
   "source": [
    "stopwords = set(stopwords.words('english'))\n",
    "removed_stopwords = [w for w in tokenized_sent if not w in stopwords]\n",
    "print(f'After removing Stopwords: {removed_stopwords}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xd4hg-SCp-o"
   },
   "source": [
    "#Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TqSIWcJ0Cg2U",
    "outputId": "5813170d-25c1-40b0-a992-76649621f2d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Text', 'processing', 'involves', 'computer', 'commands', 'invoke', 'content', 'content', 'changes', 'cursor', 'movement']\n"
     ]
    }
   ],
   "source": [
    "new_words= [word for word in removed_stopwords if word. isalnum()]\n",
    "print(new_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55hDt4kQC29y"
   },
   "source": [
    "#Filtration\n",
    "##Filtration is the process of removing other language components from the given text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q-C1tTfgDPKM"
   },
   "source": [
    "Example 01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zZTQiyeC78P",
    "outputId": "8f7d778a-5a28-44f0-986a-d582b2ec80eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Filtration:    class monitor ?\n"
     ]
    }
   ],
   "source": [
    "sentence_mix =\"कौन हे  class monitor ?\"\n",
    "print('After Filtration: ' + ''.join(list(filter(lambda x: ord(x) < 123, sentence_mix))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aGqR1KmDQ8H"
   },
   "source": [
    "Example 02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B92v029LI6DA",
    "outputId": "be4fa020-210e-4600-90f9-ebf120bf698a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Filtration:   exam   ?\n"
     ]
    }
   ],
   "source": [
    "sentence_mix =\"कितना आया exam में  ?\"\n",
    "print('After Filtration: ' + ''.join(list(filter(lambda x: ord(x) < 123, sentence_mix))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLJhlBZADh01"
   },
   "source": [
    "#Script Validation\n",
    "##Script Validation is the process of removing special characters and punctuations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QG2be8KMD0bj",
    "outputId": "cad53c67-3a7c-4118-e5eb-a088d9ee132f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After script validation: ['Text', 'processing', 'involves', 'computer', 'commands', 'invoke', 'content', '', 'content', 'changes', '', 'cursor', 'movement']\n"
     ]
    }
   ],
   "source": [
    "validated = []\n",
    "for w in removed_stopwords:\n",
    "\tvalidated.append(''.join([e for e in w if e.isalnum()]))\n",
    "print(f'After script validation: {validated}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hm5k_9TEF_Mt"
   },
   "source": [
    "#Extra_Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8uzciTG8GD2T",
    "outputId": "c4b0ec5b-dd6d-4776-f286-595ad2e64c57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing emojis: Removing emojis for text preprocessing \n",
      "After removing URL: URL of whatsapp \n"
     ]
    }
   ],
   "source": [
    "#Extra Preprocessing\n",
    "\n",
    "import re\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "emoji_sent = \"😷Removing emojis for text preprocessing 🙇🙌😱\"\n",
    "print(f'After removing emojis: {remove_emoji(emoji_sent)}')\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "url_string = 'URL of whatsapp https://web.whatsapp.com/'\n",
    "print(f'After removing URL: {remove_urls(url_string)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lO3L1WG8GTvQ"
   },
   "source": [
    "# Porter Stemmer Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "hPnUgVDiGWuY",
    "outputId": "65a13822-7847-4498-a6c9-89b53d30c3d5"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'affection'"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PorterStemmer:\n",
    "    def isCons(self, letter):\n",
    "        if letter == 'a' or letter == 'e' or letter == 'i' or letter == 'o' or letter == 'u':\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def isConsonant(self, word, i):\n",
    "        letter = word[i]\n",
    "        if self.isCons(letter):\n",
    "            if letter == 'y' and isCons(word[i-1]):\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def isVowel(self, word, i):\n",
    "        return not(isConsonant(word, i))\n",
    "\n",
    "    # *S\n",
    "    def endsWith(self, stem, letter):\n",
    "        if stem.endswith(letter):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # *v*\n",
    "    def containsVowel(self, stem):\n",
    "        for i in stem:\n",
    "            if not self.isCons(i):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # *d\n",
    "    def doubleCons(self, stem):\n",
    "        if len(stem) >= 2:\n",
    "            if self.isConsonant(stem, -1) and self.isConsonant(stem, -2):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def getForm(self, word):\n",
    "        form = []\n",
    "        formStr = ''\n",
    "        for i in range(len(word)):\n",
    "            if self.isConsonant(word, i):\n",
    "                if i != 0:\n",
    "                    prev = form[-1]\n",
    "                    if prev != 'C':\n",
    "                        form.append('C')\n",
    "                else:\n",
    "                    form.append('C')\n",
    "            else:\n",
    "                if i != 0:\n",
    "                    prev = form[-1]\n",
    "                    if prev != 'V':\n",
    "                        form.append('V')\n",
    "                else:\n",
    "                    form.append('V')\n",
    "        for j in form:\n",
    "            formStr += j\n",
    "        return formStr\n",
    "\n",
    "    def getM(self, word):\n",
    "        form = self.getForm(word)\n",
    "        m = form.count('VC')\n",
    "        return m\n",
    "\n",
    "    # *o\n",
    "    def cvc(self, word):\n",
    "        if len(word) >= 3:\n",
    "            f = -3\n",
    "            s = -2\n",
    "            t = -1\n",
    "            third = word[t]\n",
    "            if self.isConsonant(word, f) and self.isVowel(word, s) and self.isConsonant(word, t):\n",
    "                if third != 'w' and third != 'x' and third != 'y':\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def replace(self, orig, rem, rep):\n",
    "        result = orig.rfind(rem)\n",
    "        base = orig[:result]\n",
    "        replaced = base + rep\n",
    "        return replaced\n",
    "\n",
    "    def replaceM0(self, orig, rem, rep):\n",
    "        result = orig.rfind(rem)\n",
    "        base = orig[:result]\n",
    "        if self.getM(base) > 0:\n",
    "            replaced = base + rep\n",
    "            return replaced\n",
    "        else:\n",
    "            return orig\n",
    "\n",
    "    def replaceM1(self, orig, rem, rep):\n",
    "        result = orig.rfind(rem)\n",
    "        base = orig[:result]\n",
    "        if self.getM(base) > 1:\n",
    "            replaced = base + rep\n",
    "            return replaced\n",
    "        else:\n",
    "            return orig\n",
    "\n",
    "    def step1a(self, word):\n",
    "        if word.endswith('sses'):\n",
    "            word = self.replace(word, 'sses', 'ss')\n",
    "        elif word.endswith('ies'):\n",
    "            word = self.replace(word, 'ies', 'i')\n",
    "        elif word.endswith('ss'):\n",
    "            word = self.replace(word, 'ss', 'ss')\n",
    "        elif word.endswith('s'):\n",
    "            word = self.replace(word, 's', '')\n",
    "        else:\n",
    "            pass\n",
    "        return word\n",
    "\n",
    "    def step1b(self, word):\n",
    "        flag = False\n",
    "        if word.endswith('eed'):\n",
    "            result = word.rfind('eed')\n",
    "            base = word[:result]\n",
    "            if self.getM(base) > 0:\n",
    "                word = base\n",
    "                word += 'ee'\n",
    "        elif word.endswith('ed'):\n",
    "            result = word.rfind('ed')\n",
    "            base = word[:result]\n",
    "            if self.containsVowel(base):\n",
    "                word = base\n",
    "                flag = True\n",
    "        elif word.endswith('ing'):\n",
    "            result = word.rfind('ing')\n",
    "            base = word[:result]\n",
    "            if self.containsVowel(base):\n",
    "                word = base\n",
    "                flag = True\n",
    "        if flag:\n",
    "            if word.endswith('at') or word.endswith('bl') or word.endswith('iz'):\n",
    "                word += 'e'\n",
    "            elif self.doubleCons(word) and not self.endsWith(word, 'l') and not self.endsWith(word, 's') and not self.endsWith(word, 'z'):\n",
    "                word = word[:-1]\n",
    "            elif self.getM(word) == 1 and self.cvc(word):\n",
    "                word += 'e'\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "        return word\n",
    "\n",
    "    def step1c(self, word):\n",
    "        if word.endswith('y'):\n",
    "            result = word.rfind('y')\n",
    "            base = word[:result]\n",
    "            if self.containsVowel(base):\n",
    "                word = base\n",
    "                word += 'i'\n",
    "        return word\n",
    "\n",
    "    def step2(self, word):\n",
    "        if word.endswith('ational'):\n",
    "            word = self.replaceM0(word, 'ational', 'ate')\n",
    "        elif word.endswith('tional'):\n",
    "            word = self.replaceM0(word, 'tional', 'tion')\n",
    "        elif word.endswith('enci'):\n",
    "            word = self.replaceM0(word, 'enci', 'ence')\n",
    "        elif word.endswith('anci'):\n",
    "            word = self.replaceM0(word, 'anci', 'ance')\n",
    "        elif word.endswith('izer'):\n",
    "            word = self.replaceM0(word, 'izer', 'ize')\n",
    "        elif word.endswith('abli'):\n",
    "            word = self.replaceM0(word, 'abli', 'able')\n",
    "        elif word.endswith('alli'):\n",
    "            word = self.replaceM0(word, 'alli', 'al')\n",
    "        elif word.endswith('entli'):\n",
    "            word = self.replaceM0(word, 'entli', 'ent')\n",
    "        elif word.endswith('eli'):\n",
    "            word = self.replaceM0(word, 'eli', 'e')\n",
    "        elif word.endswith('ousli'):\n",
    "            word = self.replaceM0(word, 'ousli', 'ous')\n",
    "        elif word.endswith('ization'):\n",
    "            word = self.replaceM0(word, 'ization', 'ize')\n",
    "        elif word.endswith('ation'):\n",
    "            word = self.replaceM0(word, 'ation', 'ate')\n",
    "        elif word.endswith('ator'):\n",
    "            word = self.replaceM0(word, 'ator', 'ate')\n",
    "        elif word.endswith('alism'):\n",
    "            word = self.replaceM0(word, 'alism', 'al')\n",
    "        elif word.endswith('iveness'):\n",
    "            word = self.replaceM0(word, 'iveness', 'ive')\n",
    "        elif word.endswith('fulness'):\n",
    "            word = self.replaceM0(word, 'fulness', 'ful')\n",
    "        elif word.endswith('ousness'):\n",
    "            word = self.replaceM0(word, 'ousness', 'ous')\n",
    "        elif word.endswith('aliti'):\n",
    "            word = self.replaceM0(word, 'aliti', 'al')\n",
    "        elif word.endswith('iviti'):\n",
    "            word = self.replaceM0(word, 'iviti', 'ive')\n",
    "        elif word.endswith('biliti'):\n",
    "            word = self.replaceM0(word, 'biliti', 'ble')\n",
    "        return word\n",
    "\n",
    "    def step3(self, word):\n",
    "        if word.endswith('icate'):\n",
    "            word = self.replaceM0(word, 'icate', 'ic')\n",
    "        elif word.endswith('ative'):\n",
    "            word = self.replaceM0(word, 'ative', '')\n",
    "        elif word.endswith('alize'):\n",
    "            word = self.replaceM0(word, 'alize', 'al')\n",
    "        elif word.endswith('iciti'):\n",
    "            word = self.replaceM0(word, 'iciti', 'ic')\n",
    "        elif word.endswith('ful'):\n",
    "            word = self.replaceM0(word, 'ful', '')\n",
    "        elif word.endswith('ness'):\n",
    "            word = self.replaceM0(word, 'ness', '')\n",
    "        return word\n",
    "\n",
    "    def step4(self, word):\n",
    "        if word.endswith('al'):\n",
    "            word = self.replaceM1(word, 'al', '')\n",
    "        elif word.endswith('ance'):\n",
    "            word = self.replaceM1(word, 'ance', '')\n",
    "        elif word.endswith('ence'):\n",
    "            word = self.replaceM1(word, 'ence', '')\n",
    "        elif word.endswith('er'):\n",
    "            word = self.replaceM1(word, 'er', '')\n",
    "        elif word.endswith('ic'):\n",
    "            word = self.replaceM1(word, 'ic', '')\n",
    "        elif word.endswith('able'):\n",
    "            word = self.replaceM1(word, 'able', '')\n",
    "        elif word.endswith('ible'):\n",
    "            word = self.replaceM1(word, 'ible', '')\n",
    "        elif word.endswith('ant'):\n",
    "            word = self.replaceM1(word, 'ant', '')\n",
    "        elif word.endswith('ement'):\n",
    "            word = self.replaceM1(word, 'ement', '')\n",
    "        elif word.endswith('ment'):\n",
    "            word = self.replaceM1(word, 'ment', '')\n",
    "        elif word.endswith('ent'):\n",
    "            word = self.replaceM1(word, 'ent', '')\n",
    "        elif word.endswith('ou'):\n",
    "            word = self.replaceM1(word, 'ou', '')\n",
    "        elif word.endswith('ism'):\n",
    "            word = self.replaceM1(word, 'ism', '')\n",
    "        elif word.endswith('ate'):\n",
    "            word = self.replaceM1(word, 'ate', '')\n",
    "        elif word.endswith('iti'):\n",
    "            word = self.replaceM1(word, 'iti', '')\n",
    "        elif word.endswith('ous'):\n",
    "            word = self.replaceM1(word, 'ous', '')\n",
    "        elif word.endswith('ive'):\n",
    "            word = self.replaceM1(word, 'ive', '')\n",
    "        elif word.endswith('ize'):\n",
    "            word = self.replaceM1(word, 'ize', '')\n",
    "        elif word.endswith('ion'):\n",
    "            result = word.rfind('ion')\n",
    "            base = word[:result]\n",
    "            if self.getM(base) > 1 and (self.endsWith(base, 's') or self.endsWith(base, 't')):\n",
    "                word = base\n",
    "            word = self.replaceM1(word, '', '')\n",
    "        return word\n",
    "\n",
    "    def step5a(self, word):\n",
    "        if word.endswith('e'):\n",
    "            base = word[:-1]\n",
    "            if self.getM(base) > 1:\n",
    "                word = base\n",
    "            elif self.getM(base) == 1 and not self.cvc(base):\n",
    "                word = base\n",
    "        return word\n",
    "\n",
    "    def step5b(self, word):\n",
    "        if self.getM(word) > 1 and self.doubleCons(word) and self.endsWith(word, 'l'):\n",
    "            word = word[:-1]\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        word = self.step1a(word)\n",
    "        word = self.step1b(word)\n",
    "        word = self.step1c(word)\n",
    "        word = self.step2(word)\n",
    "        word = self.step3(word)\n",
    "        word = self.step4(word)\n",
    "        word = self.step5a(word)\n",
    "        word = self.step5b(word)\n",
    "        return word\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('affectionate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "q4w8ygFN1-6T",
    "outputId": "a7040388-5957-4c1c-a47c-c35986b65621"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'passion'"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('passionate')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "VOfkkjSmKCZL",
    "cmu0EqzlJZtB",
    "mDzFMChl7Skh",
    "B6px1OOO-tyt",
    "Hw5GMArm_QC6",
    "p1UiehB8B2E6",
    "0xd4hg-SCp-o"
   ],
   "name": "64_BECMPNA_Exp2_NLP.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
